{
  "Name": "Config",
  "Node": "node04-048",
  "Agent": "DDPG",
  "Number_learning_Steps": 500000,
  "Trial number": "11534",
  "Database name": "PC2_DDPG_Vctrl_single_inv_22_newTestcase",
  "Start time": "2021_08_03__07_32_51",
  "Optimierer/ Setting stuff": "Kein Const_liar_feature, hoehere Grenzen, INtergrator Gewicht als HP,Actionspace = 6, da P und I-Anteil seperate ausg\u00e4nge und im wrapper addiert werdenIntegratorzustand+used_P_Action (je um einen verzoegert) wird mit als feature uebergebenPenalties fuer action_P und action_PMehr HPs: trainfreq, batch/buffer_size, a_relu ",
  "Weitere Info": "NEUES TEST ENV - 100k steps alle 1000 resettet das vom training - zuf\u00e4lligeLast",
  "additionalInfo": "Long Holiday run",
  "penalty_I_weight": 1.132480628572647,
  "penalty_P_weight": 1.4834257541454123,
  "penalty_I_decay_start": 0.5489063567901366,
  "penalty_P_decay_start": 0.23007974811664603,
  "integrator_weight": 0.31113470671968957,
  "antiwindup_weight": 0.660818130720168,
  "learning_rate": 0.00037457864914508586,
  "lr_decay_start": 0.2750816923408933,
  "lr_decay_duration": 0.3240504611772025,
  "final_lr": 0.8356876361923928,
  "gamma": 0.9462178519540726,
  "weight_scale": 0.000852050757214834,
  "bias_scale": 0.020070268741104066,
  "alpha_relu_actor": 0.20809806015130924,
  "alpha_relu_critic": 0.006784965521936233,
  "batch_size": 261,
  "buffer_size": 386945,
  "actor_hidden_size": 25,
  "actor_number_layers": 2,
  "critic_hidden_size": 295,
  "critic_number_layers": 4,
  "noise_var": 0.023580253339050283,
  "noise_theta": 31.575020911887215,
  "training_episode_length": 2811,
  "tau": 0.002609222715831891,
  "train_freq": 2,
  "optimizer": "Adam"
}